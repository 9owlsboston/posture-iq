"""PostureIQ Tool â€” create_adoption_scorecard

Produces a structured ME5 adoption scorecard summarizing deployment status
per workload with green/yellow/red ratings, gap priorities, and time-to-green estimates.
"""

from __future__ import annotations

import json
from datetime import datetime, timezone
from typing import Any

import structlog

from src.middleware.tracing import trace_tool_call

logger = structlog.get_logger(__name__)


@trace_tool_call("create_adoption_scorecard")
async def create_adoption_scorecard(assessment_context: str) -> dict[str, Any]:
    """Produce a structured ME5 adoption scorecard.

    Args:
        assessment_context: JSON string with all assessment findings.

    Returns:
        dict with keys:
          - overall_adoption_pct: float
          - overall_status: str (green/yellow/red)
          - workload_status: dict of workload â†’ {status, coverage_pct, gaps_count}
          - top_5_gaps: list of {gap, priority, workload}
          - estimated_days_to_green: int
          - scorecard_markdown: str (human-readable markdown)
          - disclaimer: str
          - generated_at: ISO timestamp
    """
    logger.info("tool.adoption_scorecard.start")

    # TODO: In production, parse assessment_context and compute real scores
    # For now, build from mock data consistent with other tools

    # â”€â”€ Mock response for development â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    workload_status = {
        "Defender XDR": {
            "status": "yellow",
            "coverage_pct": 52.0,
            "gaps_count": 12,
            "sub_workloads": {
                "Defender for Endpoint": {"status": "yellow", "coverage_pct": 68.0},
                "Defender for Office 365": {"status": "red", "coverage_pct": 45.0},
                "Defender for Identity": {"status": "red", "coverage_pct": 30.0},
                "Defender for Cloud Apps": {"status": "yellow", "coverage_pct": 65.0},
            },
        },
        "Microsoft Purview": {
            "status": "red",
            "coverage_pct": 35.0,
            "gaps_count": 12,
            "sub_workloads": {
                "DLP Policies": {"status": "yellow", "coverage_pct": 40.0},
                "Sensitivity Labels": {"status": "red", "coverage_pct": 25.0},
                "Retention Policies": {"status": "yellow", "coverage_pct": 40.0},
                "Insider Risk Management": {"status": "red", "coverage_pct": 0.0},
            },
        },
        "Entra ID P2": {
            "status": "yellow",
            "coverage_pct": 48.0,
            "gaps_count": 16,
            "sub_workloads": {
                "Conditional Access": {"status": "yellow", "coverage_pct": 55.0},
                "PIM": {"status": "red", "coverage_pct": 30.0},
                "Identity Protection": {"status": "red", "coverage_pct": 20.0},
                "Access Reviews": {"status": "red", "coverage_pct": 0.0},
            },
        },
    }

    top_5_gaps = [
        {"gap": "Legacy authentication not blocked", "priority": "P0", "workload": "Entra ID P2"},
        {"gap": "Safe Attachments not enabled", "priority": "P0", "workload": "Defender XDR"},
        {"gap": "Identity Protection policies disabled", "priority": "P0", "workload": "Entra ID P2"},
        {"gap": "83% permanent privileged role assignments", "priority": "P1", "workload": "Entra ID P2"},
        {"gap": "Insider Risk Management not enabled", "priority": "P1", "workload": "Microsoft Purview"},
    ]

    overall_pct = sum(w["coverage_pct"] for w in workload_status.values()) / len(workload_status)

    # Generate human-readable markdown scorecard
    scorecard_md = _generate_markdown_scorecard(workload_status, top_5_gaps, overall_pct)

    result = {
        "overall_adoption_pct": round(overall_pct, 1),
        "overall_status": "red" if overall_pct < 50 else ("yellow" if overall_pct < 70 else "green"),
        "green_threshold": 70.0,
        "workload_status": workload_status,
        "top_5_gaps": top_5_gaps,
        "estimated_days_to_green": 21,
        "scorecard_markdown": scorecard_md,
        "disclaimer": (
            "âš ï¸ Generated by PostureIQ (AI-assisted) â€” review with your security "
            "team before implementing any remediation steps."
        ),
        "generated_at": datetime.now(timezone.utc).isoformat(),
    }

    logger.info(
        "tool.adoption_scorecard.complete",
        overall_adoption=result["overall_adoption_pct"],
        overall_status=result["overall_status"],
    )

    return result


def _generate_markdown_scorecard(
    workload_status: dict[str, Any],
    top_5_gaps: list[dict[str, str]],
    overall_pct: float,
) -> str:
    """Generate a human-readable markdown scorecard."""

    status_emoji = {"green": "ðŸŸ¢", "yellow": "ðŸŸ¡", "red": "ðŸ”´"}

    overall_status = "red" if overall_pct < 50 else ("yellow" if overall_pct < 70 else "green")

    lines = [
        "# PostureIQ â€” ME5 Adoption Scorecard",
        "",
        f"**Overall ME5 Adoption: {overall_pct:.1f}% {status_emoji[overall_status]}**",
        f"**Status: {'OUT OF GREEN âŒ' if overall_status != 'green' else 'GREEN âœ…'}**",
        f"**Estimated days to green: 21**",
        "",
        "---",
        "",
        "## Workload Summary",
        "",
        "| Workload | Coverage | Status |",
        "|----------|----------|--------|",
    ]

    for workload, data in workload_status.items():
        emoji = status_emoji.get(data["status"], "âšª")
        lines.append(f"| {workload} | {data['coverage_pct']:.0f}% | {emoji} {data['status'].upper()} |")

    lines.extend(["", "---", "", "## Top 5 Gaps", ""])
    for i, gap in enumerate(top_5_gaps, 1):
        lines.append(f"{i}. **[{gap['priority']}]** {gap['gap']} ({gap['workload']})")

    lines.extend([
        "",
        "---",
        "",
        "*âš ï¸ Generated by PostureIQ (AI-assisted) â€” review with your security team before implementing.*",
    ])

    return "\n".join(lines)
