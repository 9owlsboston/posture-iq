"""PostureIQ — Azure AI Content Safety middleware.

Routes LLM inputs and outputs through Azure AI Content Safety
for prompt-injection detection, harmful content filtering, and RAI compliance.

Features:
  - Filter all LLM inputs (prevent prompt injection in user queries)
  - Filter all LLM outputs (ensure remediation plans don't contain harmful content)
  - Log filter results to App Insights via structlog
  - Block responses that fail safety checks; return safe fallback
"""

from __future__ import annotations

from enum import IntEnum
from typing import Any

import structlog

from src.agent.config import settings

logger = structlog.get_logger(__name__)


# ── Severity Thresholds ────────────────────────────────────────────────


class Severity(IntEnum):
    """Azure AI Content Safety severity levels (0–6)."""

    SAFE = 0
    LOW = 2
    MEDIUM = 4
    HIGH = 6


# Block if ANY category severity reaches this level
BLOCK_THRESHOLD: int = Severity.MEDIUM  # ≥ 4 → blocked


# ── Safe Fallback Responses ────────────────────────────────────────────

SAFE_FALLBACK_INPUT = (
    "I'm unable to process this request because it was flagged by our "
    "content safety system. Please rephrase your question about your "
    "organization's security posture."
)

SAFE_FALLBACK_OUTPUT = (
    "The generated response was blocked by our content safety system. "
    "Please try again or contact your administrator. "
    "⚠️ Generated by PostureIQ (AI-assisted)."
)


# ── Content Safety Client ──────────────────────────────────────────────


def _create_content_safety_client():
    """Create an Azure AI Content Safety client.

    Returns None when the endpoint is not configured (triggers local fallback).
    """
    endpoint = settings.azure_content_safety_endpoint
    if not endpoint:
        return None

    try:
        from azure.ai.contentsafety import ContentSafetyClient

        if settings.azure_content_safety_key:
            from azure.core.credentials import AzureKeyCredential

            return ContentSafetyClient(endpoint, AzureKeyCredential(settings.azure_content_safety_key))
        else:
            from azure.identity import DefaultAzureCredential

            return ContentSafetyClient(endpoint, DefaultAzureCredential())
    except Exception as exc:
        logger.error("content_safety.client.error", error=str(exc))
        return None


# ── Core Analysis ──────────────────────────────────────────────────────


async def check_content_safety(
    text: str,
    *,
    context: str = "unknown",
) -> dict[str, Any]:
    """Check text against Azure AI Content Safety.

    Analyzes text for harmful categories: Hate, SelfHarm, Sexual, Violence.

    Args:
        text: The text content to analyze.
        context: Where this check originates (e.g. "llm_input", "llm_output").

    Returns:
        dict with keys:
          - is_safe: bool
          - categories: dict of category → severity (0–6)
          - blocked_categories: list of category names that exceeded threshold
          - reason: str | None (if blocked)
          - threshold: int (the block threshold used)
    """
    if not text or not text.strip():
        return _safe_result(context="empty_text")

    client = _create_content_safety_client()

    if client is not None:
        return await _check_with_service(client, text, context)

    # Local heuristic fallback when service is not configured
    return _check_local_heuristics(text, context)


async def _check_with_service(
    client: Any,
    text: str,
    context: str,
) -> dict[str, Any]:
    """Analyze text using the Azure AI Content Safety service."""
    try:
        from azure.ai.contentsafety.models import AnalyzeTextOptions

        request = AnalyzeTextOptions(text=text[:10000])  # API limit
        response = client.analyze_text(request)

        # SDK returns categories_analysis: list[TextCategoriesAnalysis]
        # Each item has .category (TextCategory enum) and .severity (int 0-6)
        category_map = {
            "Hate": "hate",
            "SelfHarm": "self_harm",
            "Sexual": "sexual",
            "Violence": "violence",
        }

        categories: dict[str, int] = {
            "hate": 0,
            "self_harm": 0,
            "sexual": 0,
            "violence": 0,
        }

        for item in response.get("categories_analysis", []):
            cat_name = str(item.get("category", ""))
            severity = int(item.get("severity", 0))
            mapped = category_map.get(cat_name, cat_name.lower())
            if mapped in categories:
                categories[mapped] = severity

        blocked = [cat for cat, sev in categories.items() if sev >= BLOCK_THRESHOLD]
        is_safe = len(blocked) == 0

        reason = None
        if not is_safe:
            reason = f"Blocked categories: {', '.join(blocked)}"

        logger.info(
            "content_safety.check.complete",
            is_safe=is_safe,
            categories=categories,
            blocked_categories=blocked,
            context=context,
            source="azure_service",
        )

        return {
            "is_safe": is_safe,
            "categories": categories,
            "blocked_categories": blocked,
            "reason": reason,
            "threshold": BLOCK_THRESHOLD,
        }

    except Exception as exc:
        logger.error(
            "content_safety.service.error",
            error=str(exc),
            context=context,
        )
        # Fail open with a warning — don't block legitimate requests
        # due to service issues, but log for monitoring
        return _safe_result(context=f"service_error:{context}")


def _check_local_heuristics(
    text: str,
    context: str,
) -> dict[str, Any]:
    """Local heuristic content check when the Azure service is unavailable.

    This is NOT a replacement for the real Content Safety service —
    it catches only obvious pattern-based issues.
    """
    lower = text.lower()
    categories: dict[str, int] = {
        "hate": 0,
        "self_harm": 0,
        "sexual": 0,
        "violence": 0,
    }

    # Very basic keyword heuristics for dev/test; real filtering uses the service
    hate_indicators = [
        "kill all",
        "hate all",
        "death to",
        "exterminate",
    ]
    violence_indicators = [
        "how to attack",
        "how to hack into",
        "destroy the",
        "bomb",
        "weapon",
    ]

    if any(phrase in lower for phrase in hate_indicators):
        categories["hate"] = Severity.MEDIUM
    if any(phrase in lower for phrase in violence_indicators):
        categories["violence"] = Severity.MEDIUM

    blocked = [cat for cat, sev in categories.items() if sev >= BLOCK_THRESHOLD]
    is_safe = len(blocked) == 0
    reason = f"Blocked categories: {', '.join(blocked)}" if not is_safe else None

    logger.info(
        "content_safety.check.complete",
        is_safe=is_safe,
        categories=categories,
        blocked_categories=blocked,
        context=context,
        source="local_heuristics",
    )

    return {
        "is_safe": is_safe,
        "categories": categories,
        "blocked_categories": blocked,
        "reason": reason,
        "threshold": BLOCK_THRESHOLD,
    }


def _safe_result(context: str = "") -> dict[str, Any]:
    """Return a result indicating the content is safe."""
    return {
        "is_safe": True,
        "categories": {"hate": 0, "self_harm": 0, "sexual": 0, "violence": 0},
        "blocked_categories": [],
        "reason": None,
        "threshold": BLOCK_THRESHOLD,
    }


# ── LLM I/O Filters ───────────────────────────────────────────────────


async def filter_llm_input(user_input: str) -> dict[str, Any]:
    """Filter LLM input — checks both content safety and prompt injection.

    Call this BEFORE sending user input to the LLM.

    Returns:
        dict with:
          - is_safe: bool (True if both checks pass)
          - content_safety: dict (content safety result)
          - prompt_injection: dict (prompt injection result)
          - safe_fallback: str (fallback message if blocked)
    """
    content_result = await check_content_safety(user_input, context="llm_input")
    injection_result = await check_prompt_injection(user_input)

    is_safe = content_result["is_safe"] and injection_result["is_safe"]

    if not is_safe:
        logger.warning(
            "content_safety.llm_input.blocked",
            content_safe=content_result["is_safe"],
            injection_safe=injection_result["is_safe"],
            input_preview=user_input[:100],
        )

    return {
        "is_safe": is_safe,
        "content_safety": content_result,
        "prompt_injection": injection_result,
        "safe_fallback": SAFE_FALLBACK_INPUT if not is_safe else None,
    }


async def filter_llm_output(llm_output: str) -> dict[str, Any]:
    """Filter LLM output — checks content safety on generated response.

    Call this AFTER receiving the LLM response, BEFORE returning to the user.

    Returns:
        dict with:
          - is_safe: bool
          - content_safety: dict
          - safe_fallback: str (fallback message if blocked)
          - filtered_output: str (original output if safe, fallback if not)
    """
    content_result = await check_content_safety(llm_output, context="llm_output")

    if not content_result["is_safe"]:
        logger.warning(
            "content_safety.llm_output.blocked",
            blocked_categories=content_result.get("blocked_categories"),
            output_preview=llm_output[:100],
        )

    return {
        "is_safe": content_result["is_safe"],
        "content_safety": content_result,
        "safe_fallback": SAFE_FALLBACK_OUTPUT if not content_result["is_safe"] else None,
        "filtered_output": llm_output if content_result["is_safe"] else SAFE_FALLBACK_OUTPUT,
    }


# ── Prompt Injection Detection ─────────────────────────────────────────

# Patterns that indicate prompt injection / jailbreak attempts
INJECTION_PATTERNS: list[str] = [
    "ignore previous instructions",
    "ignore your system prompt",
    "ignore all previous",
    "ignore the above",
    "disregard all",
    "disregard your",
    "disregard previous",
    "forget your instructions",
    "forget all previous",
    "override your",
    "you are now",
    "pretend you are",
    "act as if you are",
    "new instructions:",
    "system prompt:",
    "you must now",
    "from now on you",
    "jailbreak",
    "do anything now",
    "dan mode",
]


async def check_prompt_injection(user_input: str) -> dict[str, Any]:
    """Check user input for prompt injection / jailbreak attempts.

    Uses Azure AI Content Safety prompt shield when available,
    with local pattern matching as a fallback.

    Args:
        user_input: The user's query to analyze.

    Returns:
        dict with keys:
          - is_safe: bool
          - attack_detected: bool
          - matched_pattern: str | None (which pattern matched)
          - reason: str | None (if blocked)
    """
    if not user_input:
        return {
            "is_safe": True,
            "attack_detected": False,
            "matched_pattern": None,
            "reason": None,
        }

    lower_input = user_input.lower()

    # Check each injection pattern
    matched: str | None = None
    for pattern in INJECTION_PATTERNS:
        if pattern in lower_input:
            matched = pattern
            break

    attack_detected = matched is not None

    result: dict[str, Any] = {
        "is_safe": not attack_detected,
        "attack_detected": attack_detected,
        "matched_pattern": matched,
        "reason": f"Prompt injection detected: '{matched}'" if attack_detected else None,
    }

    if attack_detected:
        logger.warning(
            "content_safety.prompt_injection.detected",
            matched_pattern=matched,
            input_preview=user_input[:100],
        )

    return result
