"""PostureIQ — Responsible AI (RAI) utilities.

Centralizes confidence-score assignment and disclaimer watermarks that
are applied to every AI-generated scorecard and remediation plan.

Confidence Levels:
  - **high**: Standard remediation with full assessment data available
  - **medium**: Some data sources missing or remediation is context-dependent
  - **low**: Limited data, speculative recommendation

Disclaimer:
  Every AI-generated output MUST include the standard watermark via
  :func:`add_disclaimer`.
"""

from __future__ import annotations

from typing import Any

import structlog

logger = structlog.get_logger(__name__)


# ── Disclaimer Watermark ───────────────────────────────────────────────

DISCLAIMER_TEXT = "⚠️ Generated by PostureIQ (AI-assisted) — review with your security team before implementing."

DISCLAIMER_TEXT_MARKDOWN = (
    "*⚠️ Generated by PostureIQ (AI-assisted) — review with your security team before implementing.*"
)

DISCLAIMER_TEXT_SCRIPTS = (
    "⚠️ Generated by PostureIQ (AI-assisted) — review with your security team "
    "before implementing. Scripts should be tested in a non-production environment first."
)


def add_disclaimer(
    output: dict[str, Any],
    *,
    variant: str = "default",
) -> dict[str, Any]:
    """Add the standard RAI disclaimer watermark to an output dict.

    Always sets the ``disclaimer`` key, regardless of whether one already exists.

    Args:
        output: The tool output dictionary.
        variant: ``"default"`` | ``"markdown"`` | ``"scripts"``

    Returns:
        The same dict with the ``disclaimer`` key set.
    """
    disclaimers = {
        "default": DISCLAIMER_TEXT,
        "markdown": DISCLAIMER_TEXT_MARKDOWN,
        "scripts": DISCLAIMER_TEXT_SCRIPTS,
    }
    output["disclaimer"] = disclaimers.get(variant, DISCLAIMER_TEXT)
    return output


def has_disclaimer(output: dict[str, Any]) -> bool:
    """Check whether the output already contains a valid disclaimer."""
    disclaimer = output.get("disclaimer", "")
    return (
        isinstance(disclaimer, str) and "Generated by" in disclaimer and "review with your security team" in disclaimer
    )


# ── Confidence Scoring ────────────────────────────────────────────────

CONFIDENCE_LEVELS = ("high", "medium", "low")


def assign_confidence(
    *,
    data_available: bool = True,
    data_source: str = "mock",
    data_completeness_pct: float = 100.0,
    is_standard_remediation: bool = True,
) -> str:
    """Compute a confidence level for a remediation recommendation.

    Factors:
      - Whether real data is available (vs. mock)
      - Completeness of the data (how many sources contributed)
      - Whether the remediation is a standard / well-known action

    Args:
        data_available: True if the recommendation is backed by actual data.
        data_source: ``"live"`` | ``"openai"`` | ``"mock"``
        data_completeness_pct: 0–100, how complete the input data is.
        is_standard_remediation: True for well-known best-practice actions.

    Returns:
        ``"high"`` | ``"medium"`` | ``"low"``
    """
    if data_source == "mock":
        return "low"

    if not data_available:
        return "low"

    if data_completeness_pct >= 80.0 and is_standard_remediation:
        return "high"

    if data_completeness_pct >= 50.0:
        return "medium"

    return "low"


def apply_confidence_to_steps(
    steps: list[dict[str, Any]],
    *,
    data_source: str = "mock",
    data_completeness_pct: float = 100.0,
) -> list[dict[str, Any]]:
    """Apply confidence scores to a list of remediation steps.

    Each step can already have a ``confidence`` field (set by the LLM);
    this function overrides it only if it would be *lowered* (never raises
    confidence beyond what the data supports).

    Args:
        steps: List of remediation step dicts.
        data_source: ``"live"`` | ``"openai"`` | ``"mock"``
        data_completeness_pct: 0–100.

    Returns:
        The same list with updated ``confidence`` fields.
    """
    confidence_order = {"high": 2, "medium": 1, "low": 0}

    for step in steps:
        is_standard = step.get("priority") in ("P0", "P1")
        computed = assign_confidence(
            data_available=True,
            data_source=data_source,
            data_completeness_pct=data_completeness_pct,
            is_standard_remediation=is_standard,
        )

        existing = step.get("confidence")
        if existing is None:
            # No confidence set yet — assign computed value
            step["confidence"] = computed
        elif confidence_order.get(computed, 0) < confidence_order.get(existing, 0):
            # Only lower confidence, never raise it
            step["confidence"] = computed

    return steps


# ── Combined RAI Post-Processing ───────────────────────────────────────


def apply_rai_post_processing(
    output: dict[str, Any],
    *,
    data_source: str = "mock",
    data_completeness_pct: float = 100.0,
    disclaimer_variant: str = "default",
) -> dict[str, Any]:
    """Apply all RAI post-processing to a tool output.

    1. Add disclaimer watermark
    2. Apply confidence scoring to any ``steps`` list

    Args:
        output: The tool output dictionary.
        data_source: Data provenance.
        data_completeness_pct: 0–100.
        disclaimer_variant: Disclaimer variant to use.

    Returns:
        The post-processed output dict.
    """
    add_disclaimer(output, variant=disclaimer_variant)

    if "steps" in output and isinstance(output["steps"], list):
        apply_confidence_to_steps(
            output["steps"],
            data_source=data_source,
            data_completeness_pct=data_completeness_pct,
        )

    return output
